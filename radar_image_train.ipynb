{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "radar_image_train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZUDExm2h7sIsfbEN08WZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SYEON9/radar_image_predict/blob/main/radar_image_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bq7SPaKk8Nk"
      },
      "source": [
        "#drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztgYJUovlJtH"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import zipfile\n",
        "\n",
        "\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, BatchNormalization, concatenate, ConvLSTM2D, Conv3D\n",
        "from tensorflow.keras import Model, layers, optimizers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TWccOczlbUa"
      },
      "source": [
        "#data load\n",
        "train_file = zipfile.ZipFile('/content/drive/MyDrive/235646_공공데이터 활용 수력 댐 강우예측 AI 경진대회_data/train.zip')\n",
        "train_file.extractall('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5zK-lYgle99"
      },
      "source": [
        "#이미지 가져오기\n",
        "#train데이터의 이름을 train_files라는 변수에 하나의 리스트로 넣음.\n",
        "train_files = glob.glob('/content/train/*.npy')\n",
        "len(train_files) #전체 데이터 개수 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hntsp_6nlfzL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8hukCUK-QhT"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeuosfj8-R23"
      },
      "source": [
        "#1번째 데이터만 불러와서 데이터를 확인하자.\n",
        "data_1st = np.load(train_files[0])\n",
        "data_1st.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoHetbSm-VDB"
      },
      "source": [
        "#시각화해서 다시 확인하자. \n",
        "#color map 지정\n",
        "color_map = plt.cm.get_cmap('RdBu')  #\"RdBu\"색으로 이미지를 칠함. \n",
        "color_map = color_map.reversed()  #지정된 색을 뒤집는다. \n",
        "\n",
        "#시각화\n",
        "plt.style.use('fivethirtyeight')  #스타일 시트 지정.\n",
        "plt.figure(figsize = (20,20)) #창의 크기를 (20, 20)으로 설정. \n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(data_1st[:,:,i], cmap = color_map)\n",
        "\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.imshow(data_1st[:,:,-1], cmap=color_map)\n",
        "plt.show()\n",
        "\n",
        "#데이터는 10분 간격의 5개의 이미지로 구성되어 있다. \n",
        "#그러나 마지막 데이터는 label이므로 모델을 학습할 때에는 앞의 4개만 사용해야 한다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvGlrEbw-V7T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn815RQL-Wmx"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hThZ9NyR-UOu"
      },
      "source": [
        "#데이터 불러오기\n",
        "#5개의 이미지 데이터를 4개의 train 데이터와 1개의 label 데이터로 분리한다. \n",
        "def trainGenerator():\n",
        "    for file in train_files:\n",
        "        dataset = np.load(file)\n",
        "        target= dataset[:,:,-1].reshape(1,1,120,120,1)\n",
        "        remove_minus = np.where(target < 0, 0, target)\n",
        "        feature = dataset[:,:,:4].reshape(1,1,120,120,4)\n",
        "\n",
        "        yield (feature, remove_minus)\n",
        "        \n",
        "#메모리 문제로 1개의 batch만큼만 올려 처리한다. \n",
        "#이미지 3채널을 5채널로 변경\n",
        "#batch size를 256으로 설정. \n",
        "#prefetch를 사용하여 미리 데이터를 가져옴. -> 메모리 대기시간을 줄일 수 있음.\n",
        "train_data = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([1,1,120,120,4]),tf.TensorShape([1,1,120,120,1])))\n",
        "train_dataset = train_data.batch(256).prefetch(1)\n",
        "train_d = train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQiWODu9-ldk"
      },
      "source": [
        "def trainGenerator2():\n",
        "    for file in train_files:\n",
        "        dataset = np.load(file)\n",
        "        target= dataset[:,:,-1].reshape(1,1,120,120,1)\n",
        "        remove_minus = np.where(target < 0, 0, target)\n",
        "        feature = dataset[:,:,:4].reshape(1,1,120,120,4)\n",
        "\n",
        "        yield (feature, remove_minus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtgzgKfo-mM5"
      },
      "source": [
        "def trainGenerator():\n",
        "    for file in tr_file:\n",
        "        dataset = np.load(file)\n",
        "        target= dataset[:,:,-1].reshape(1,1,120,120,1)        \n",
        "        remove_minus = np.where(target < 0, 0, target)\n",
        "        feature = dataset[:,:,:4].reshape(1,1,120,120,4)\n",
        "\n",
        "\n",
        "\n",
        "        yield (feature, remove_minus)\n",
        "\n",
        "def valGenerator():\n",
        "    for file in val_file:\n",
        "        dataset = np.load(file)\n",
        "        target= dataset[:,:,-1].reshape(1,1,120,120,1)        \n",
        "        remove_minus = np.where(target < 0, 0, target)\n",
        "        feature = dataset[:,:,:4].reshape(1,1,120,120,4)\n",
        "\n",
        "\n",
        "        yield (feature, remove_minus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m6-rafb-mmu"
      },
      "source": [
        "# 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj5-qfhm-uhY"
      },
      "source": [
        "#model\n",
        "inp = layers.Input(shape = (1,120,120,4))\n",
        "\n",
        "x = layers.ConvLSTM2D(64, kernel_size=(5,5), padding = 'same', return_sequences = True, activation = 'relu',)(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(64, kernel_size=(5,5), padding = 'same', return_sequences = True, activation = 'relu',)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(64, kernel_size=(5,5), padding = 'same', return_sequences = True, activation = 'relu',)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(64, kernel_size=(5,5), padding = 'same', return_sequences = True, activation = 'relu',)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "x = layers.Conv3D(filters = 1, kernel_size = (5,5,5), activation = 'sigmoid', padding='same')(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTU4XZIH-8sB"
      },
      "source": [
        "model = keras.models.Model(inp,x)\n",
        "model.compile(loss = 'mae', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHLqA5Sy--RG"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdtvIXmF_AF0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iablV7WX_AOc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mmmhkts_A_p"
      },
      "source": [
        "# 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4YzDFPf_IRN"
      },
      "source": [
        "#필요한 라이브러리\n",
        "import gc\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA7yor9a_urA"
      },
      "source": [
        "#k-fold cross validation \n",
        "kf = KFold(n_splits=5, random_state=42)\n",
        "for fold, (train, val) in enumerate(kf.split(train_files)):\n",
        "    val_file = train_files[val]\n",
        "    tr_file = train_files[train]\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([1,1,120,120,4]),tf.TensorShape([1,1,120,120,1])))\n",
        "    train_dataset = train_dataset.shuffle(256)\n",
        "\n",
        "    train_dataset = train_dataset.batch(128).prefetch(1)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_generator(valGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
        "    val_dataset = val_dataset.batch(128).prefetch(1)\n",
        "\n",
        "\n",
        "    callbacks = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.2, patience=2, verbose=0, mode='min',\n",
        "        min_delta=0.0001, cooldown=0, min_lr=0\n",
        "    )\n",
        "\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(root_path,f'{fold}-rain.h5'), monitor='val_loss', verbose=0, save_best_only=True,\n",
        "        save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    \n",
        "    model.fit(train_dataset, epochs = 1, verbose=1, validation_data=val_dataset, callbacks=[callbacks, sv])\n",
        "\n",
        "    \n",
        "    model.save('/content/convlstm_model_'+str(idx)+'_lastEpoch.h5')\n",
        "    idx += 1\n",
        "\n",
        "   scores = model.evaluate(val_dataset, batch_size = BATCH_SIZE)\n",
        "   all_scores.append(scores)\n",
        "\n",
        "   pred = model.predict(test_x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7PDWCHCEr0R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYq7ZVIVHh_w"
      },
      "source": [
        "t = trainGenerator2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGlnSiAwI9Ke"
      },
      "source": [
        "t.__dir__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEdNefRIi8s"
      },
      "source": [
        "t.__sizeof__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csC2jhhoIt4U"
      },
      "source": [
        "def trainGenerator2():\n",
        "    for file in train_files:\n",
        "        dataset = np.load(file)\n",
        "        target= dataset[:,:,-1].reshape(1,1,120,120,1)\n",
        "        remove_minus = np.where(target < 0, 0, target)\n",
        "        feature = dataset[:,:,:4].reshape(1,1,120,120,4)\n",
        "\n",
        "        yield (feature, remove_minus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elTEdPtnhQZN"
      },
      "source": [
        "#generator를 사용하지 않고 데이터를 불러오기.\n",
        "\n",
        "train_x, train_y = []\n",
        "\n",
        "for file in train_files:  #mapping하는게 좋을까?\n",
        "    dataset = np.load(file)\n",
        "    target = dataset[:,:,-1].reshape(1,1,120,120,1)\n",
        "    remove_minus = np.where(target<0,0,target)\n",
        "    feature = dataset[:,:,4].reshape(1,1,120,120,4)\n",
        "\n",
        "    train_x,append(feature)\n",
        "    train_y.append(feature)\n",
        "\n",
        "\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCpO_oOIdDmy"
      },
      "source": [
        "#train, test 데이터로 슬라이싱 할 정수 만들기\n",
        "def permutation_train_test_split(x, y, test_size=0.2, shuffle = True, random_state = 1004):\n",
        "    test_num = int(x.shape[0]*test_size)\n",
        "    train_num = x.shape[0] - test_num\n",
        "\n",
        "test_num =  random.sample(train_files, int(len(train_files)*0.3))\n",
        "test_x = train_x[test_num]\n",
        "test_y = train_y[test_num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msV0EKiEaStP"
      },
      "source": [
        "train_sample = []\n",
        "for file in train_files"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}